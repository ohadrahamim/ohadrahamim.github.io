<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://avivsham.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://avivsham.github.io/" rel="alternate" type="text/html" /><updated>2023-02-17T12:32:16+00:00</updated><id>https://avivsham.github.io/feed.xml</id><title type="html">Aviv Shamsian</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>Aviv Shamsian</name></author><entry><title type="html">Post: Real-Time Semantic Segmentation</title><link href="https://avivsham.github.io/blog/enet/" rel="alternate" type="text/html" title="Post: Real-Time Semantic Segmentation" /><published>2020-10-08T00:00:00+00:00</published><updated>2020-10-08T00:00:00+00:00</updated><id>https://avivsham.github.io/blog/enet</id><content type="html" xml:base="https://avivsham.github.io/blog/enet/">&lt;p&gt;#ENet model implementation
In this blog I will present an overview on image segmentation and the reproduction of ENet paper.&lt;/p&gt;

&lt;p&gt;Link to the paper: https://arxiv.org/pdf/1606.02147.pdf&lt;/p&gt;

&lt;p&gt;Link to the repository: https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation&lt;/p&gt;

&lt;h2 id=&quot;image-segmentation-overview&quot;&gt;Image Segmentation Overview&lt;/h2&gt;
&lt;p&gt;In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as super-pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. 
Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.
There are various techniques in the field of image segmentation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Region based segmentation&lt;/li&gt;
  &lt;li&gt;Edge detection segmentation&lt;/li&gt;
  &lt;li&gt;Segmentation based on clustering&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;classical-approaches-to-image-segmentation&quot;&gt;Classical Approaches to Image Segmentation&lt;/h2&gt;
&lt;p&gt;In the past, various algorithms were proposed to perform image segmentation, and these are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Thresholding technique&lt;/strong&gt; - the main objective of technique is to determine an optimal threshold value of an image.
The pixels whose intensity values exceed the threshold value are transformed to 1 and the others to 0 creating a binary image.
methods to select the threshold value: Otsu, K-means clustering and maximum entropy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Motion and interactive segmentation&lt;/strong&gt; - this technique is based on motion in the image to perform the segmentation.
The idea is intuitive, check the differences between pair of images assuming the object is moving, the difference will be exactly that object.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Edge detection&lt;/strong&gt; -  includes a variety of mathematical methods that aim at identifying points in a digital image at which the image brightness changes sharply or, more formally, has discontinuities. 
the correlation between region boundaries and edges is high, therefore usually edge detection is used as the base of another segmentation technique.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Region-growing methods&lt;/strong&gt; - rely mainly on the assumption that the neighboring pixels within one region have similar values. The common procedure is to compare one pixel with its neighbors. If a similarity criterion is satisfied, the pixel can be set to belong to the cluster as one or more of its neighbors. 
The selection of the similarity criterion is significant, and the results are influenced by noise in all instances.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are more methods didn’t mentioned above used for image segmentation such as: dual clustering, fast marching, watershed transformation and more.&lt;/p&gt;

&lt;h2 id=&quot;deep-learning-models-for-image-segmentation&quot;&gt;Deep Learning Models for Image Segmentation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;UNet&lt;/strong&gt; - The u-net is convolutional network architecture for fast and precise segmentation of images.
Up to now it has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. 
It has won the Grand Challenge for Computer-Automated Detection of Caries in Bitewing Radiography at ISBI 2015, and it has won the Cell Tracking Challenge at ISBI 2015 on the two most challenging transmitted light microscopy categories (Phase contrast and DIC microscopy) by a large margin (See also our announcement).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SegNet&lt;/strong&gt; - SegNet is assembled from encoders and decoders but without fully connected layers.
SegNet is a fully convolutional network (FCN) with 13 VGG16 convolutional layers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://saytosid.github.io/images/segnet/Complete%20architecture.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mask R-CNN&lt;/strong&gt; - Faster R-CNN uses a CNN feature extractor to extract image features. Then it uses a CNN region proposal network to create region of interests (RoIs). We apply RoI pooling to warp them into fixed dimension. 
It is then feed into fully connected layers to make classification and boundary box prediction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1250/1*0cxB2pAxQ0A7AhTl-YT2JQ.jpeg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Full-Resolution Residual Networks (FRRN)&lt;/strong&gt; - FRRN uses additional processing steps
must be performed in order to obtain pixel-accurate segmentation masks at the full image resolution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1000/1*LlYK2Pjemx3kNC61yVV-yA.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pyramid Scene Parsing Network (PSPNet)&lt;/strong&gt; - Full-Resolution Residual Networks were really  computationally intensive and using them on full scale photos was really slow.
In order to deal with this problem PSPNet applies 4 different max pooling operation with 4 different window sizes and strides. Using the max pooling layers able to extract feature information from different scales much more efficiently.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1000/1*REgHs3PeemO3TIuyE46iRg.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DeepLabv3+&lt;/strong&gt; - former networks are able to encode multi scale contextual information by using filters and pooling operations in different rates.
newer networks can capture sharper object boundaries by recovering the spatial information. DeepLabv3+ combines these two approaches.
DeepLabv3+ uses both encoder decoder and spatial pyramid pooling modules.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1000/1*MFchBd4c8ZEgE3qtbnTznw.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;enet-implementation&quot;&gt;ENet Implementation&lt;/h2&gt;
&lt;p&gt;ENet (Efficient Neural Network) gives the ability to perform pixel-wise semantic segmentation in real-time. ENet is upto 18x faster, requires 75x less FLOPs, has 79x less parameters and provides similar or better accuracy to existing models (according to 2016). Tested on CamVid, CityScapes and SUN datasets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1000/1*CKuZqyLSc4U8BjG3sWZHew.png&quot; alt=&quot;model architechture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model architechture consists of inital block and five bottlenecks. The first three bottlenecks are used for encoding the input image and the other two for decoding it.&lt;/p&gt;

&lt;p&gt;Each bottleneck module consists of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1x1 projection that reduces the dimensionality&lt;/li&gt;
  &lt;li&gt;A main convolution layer (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conv&lt;/code&gt;) (either — regular, dilated or full) (3x3)&lt;/li&gt;
  &lt;li&gt;1x1 expansion&lt;/li&gt;
  &lt;li&gt;and they place Batch Normalization and PReLU between all convolutional layers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If the bottleneck is downsampling, a max pooling layer is added to the main branch. Also, the first 1x1 projection is replaced with 2x2 convolution with stride=2.&lt;/p&gt;

&lt;p&gt;They zero pad the activations to match the number of feature maps.&lt;/p&gt;

&lt;p&gt;The conv is sometimes asymmetric convolution i.e. a sequence of 5 * 1 and 1 * 5 convolutions.&lt;/p&gt;

&lt;p&gt;For the regularizer they use Spatial Dropout:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;with p = 0.01 before bottleneck2.0&lt;/li&gt;
  &lt;li&gt;with p = 0.1 afterwards&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2600/1*RWYNKupbSTFFTOU7_7-sAg.png&quot; alt=&quot;bottleneck&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;enet-model-results&quot;&gt;Enet model Results&lt;/h4&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26242097/51782315-4b88d300-214c-11e9-9c92-3444c6582a80.png&quot; alt=&quot;enet infer 1&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26242097/51782341-a02c4e00-214c-11e9-8566-f2092ddad086.png&quot; alt=&quot;enet infer 4&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26242097/51782371-01542180-214d-11e9-80b8-55807f83f776.png&quot; alt=&quot;enet infer 6&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26242097/51782353-c3ef9400-214c-11e9-8c66-276795c83f08.png&quot; alt=&quot;enet infer 5&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26242097/51782324-6b1ffb80-214c-11e9-9f92-741954699f4d.png&quot; alt=&quot;enet infer 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you want to train ENet model and reproduce the results in one go you can open the notebook at this &lt;a href=&quot;https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation/blob/master/ENet-Real%20Time%20Semantic%20Segmentation.ipynb&quot;&gt;link&lt;/a&gt; and run it. &lt;strong&gt;No downloads needed just run and enjoy!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you liked it, don’t forget to clap, star and fork the project!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation&lt;/p&gt;</content><author><name>Aviv Shamsian</name></author><category term="blog" /><category term="Computer Vision" /><category term="Machine Learning" /><category term="Deep Learning" /><summary type="html">#ENet model implementation In this blog I will present an overview on image segmentation and the reproduction of ENet paper.</summary></entry></feed>